{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.22.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.11/site-packages (from torchvision) (1.26.4)\n",
      "Collecting torch==2.7.0 (from torchvision)\n",
      "  Downloading torch-2.7.0-cp311-none-macosx_11_0_arm64.whl.metadata (29 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from torch==2.7.0->torchvision) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/ayush1/.local/lib/python3.11/site-packages (from torch==2.7.0->torchvision) (4.12.2)\n",
      "Collecting sympy>=1.13.3 (from torch==2.7.0->torchvision)\n",
      "  Downloading sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.11/site-packages (from torch==2.7.0->torchvision) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from torch==2.7.0->torchvision) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.11/site-packages (from torch==2.7.0->torchvision) (2023.10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from sympy>=1.13.3->torch==2.7.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->torch==2.7.0->torchvision) (2.1.3)\n",
      "Downloading torchvision-0.22.0-cp311-cp311-macosx_11_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.7.0-cp311-none-macosx_11_0_arm64.whl (68.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.6/68.6 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sympy, torch, torchvision\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.1\n",
      "    Uninstalling sympy-1.13.1:\n",
      "      Successfully uninstalled sympy-1.13.1\n",
      "\u001b[33m  WARNING: The script isympy is installed in '/Users/ayush1/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.6.0\n",
      "    Uninstalling torch-2.6.0:\n",
      "      Successfully uninstalled torch-2.6.0\n",
      "\u001b[33m  WARNING: The scripts torchfrtrace and torchrun are installed in '/Users/ayush1/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed sympy-1.13.3 torch-2.7.0 torchvision-0.22.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pycocotools\n",
      "  Downloading pycocotools-2.0.8-cp311-cp311-macosx_10_9_universal2.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from pycocotools) (3.8.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.11/site-packages (from pycocotools) (1.26.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib>=2.1.0->pycocotools) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib>=2.1.0->pycocotools) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib>=2.1.0->pycocotools) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib>=2.1.0->pycocotools) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib>=2.1.0->pycocotools) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib>=2.1.0->pycocotools) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib>=2.1.0->pycocotools) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib>=2.1.0->pycocotools) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.16.0)\n",
      "Downloading pycocotools-2.0.8-cp311-cp311-macosx_10_9_universal2.whl (162 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.9/162.9 kB\u001b[0m \u001b[31m125.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pycocotools\n",
      "Successfully installed pycocotools-2.0.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pycocotools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.77s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.04s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2686 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 185\u001b[0m\n\u001b[1;32m    182\u001b[0m model \u001b[38;5;241m=\u001b[39m SimpleDetector(num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    183\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[0;32m--> 185\u001b[0m train(model, train_loader, optimizer, device, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m    186\u001b[0m evaluate(model, val_loader, device)\n",
      "Cell \u001b[0;32mIn[8], line 119\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, device, epochs)\u001b[0m\n\u001b[1;32m    117\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    118\u001b[0m cls_logits, bbox_preds \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[0;32m--> 119\u001b[0m loss \u001b[38;5;241m=\u001b[39m detection_loss(cls_logits, bbox_preds, targets[\u001b[38;5;241m0\u001b[39m])  \u001b[38;5;66;03m# one by one\u001b[39;00m\n\u001b[1;32m    120\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    121\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[8], line 93\u001b[0m, in \u001b[0;36mdetection_loss\u001b[0;34m(cls_logits, bbox_preds, targets)\u001b[0m\n\u001b[1;32m     91\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m cls_logits\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     92\u001b[0m cls_logits_flat \u001b[38;5;241m=\u001b[39m cls_logits\u001b[38;5;241m.\u001b[39mmean([\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m])  \u001b[38;5;66;03m# (B, num_classes)\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([t[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(t[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m])  \u001b[38;5;66;03m# safer access\u001b[39;00m\n\u001b[1;32m     94\u001b[0m boxes \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([t[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(t[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m])     \u001b[38;5;66;03m# safer access\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m!=\u001b[39m cls_logits_flat\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m):\n",
      "Cell \u001b[0;32mIn[8], line 93\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     91\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m cls_logits\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     92\u001b[0m cls_logits_flat \u001b[38;5;241m=\u001b[39m cls_logits\u001b[38;5;241m.\u001b[39mmean([\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m])  \u001b[38;5;66;03m# (B, num_classes)\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([t[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(t[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m])  \u001b[38;5;66;03m# safer access\u001b[39;00m\n\u001b[1;32m     94\u001b[0m boxes \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([t[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(t[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m])     \u001b[38;5;66;03m# safer access\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m!=\u001b[39m cls_logits_flat\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m):\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "# ----------- Step 0: Required Libraries -----------\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image, ImageDraw\n",
    "from pycocotools.coco import COCO\n",
    "from tqdm import tqdm\n",
    "import torchvision.ops as ops\n",
    "\n",
    "# ----------- Step 1: Custom COCO Dataset Loader -----------\n",
    "\n",
    "class FLIRCOCODataset(Dataset):\n",
    "    def __init__(self, img_dir, annot_file, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.coco = COCO(annot_file)\n",
    "        self.image_ids = list(self.coco.imgs.keys())\n",
    "        self.transform = transform\n",
    "\n",
    "        self.label_map = {\n",
    "            1: 1,  # person\n",
    "            2: 2, 3: 2, 4: 2, 6: 2, 8: 2, 13: 2  # vehicles\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        image_info = self.coco.loadImgs(image_id)[0]\n",
    "        img_path = os.path.join(self.img_dir, \"data\", os.path.basename(image_info['file_name']))\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=image_id)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        for ann in anns:\n",
    "            cat_id = ann['category_id']\n",
    "            if cat_id in self.label_map:\n",
    "                labels.append(self.label_map[cat_id])\n",
    "                x, y, w, h = ann['bbox']\n",
    "                boxes.append([x, y, x + w, y + h])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        target = {\"boxes\": torch.tensor(boxes, dtype=torch.float32),\n",
    "                  \"labels\": torch.tensor(labels, dtype=torch.int64)}\n",
    "\n",
    "        return image, target\n",
    "\n",
    "# ----------- Step 2: Model (Simple CNN + Detection Head) -----------\n",
    "\n",
    "class SimpleDetector(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(SimpleDetector, self).__init__()\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, stride=2, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, 3, stride=2, padding=1), nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.cls_head = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(128, num_classes, 1)\n",
    "        )\n",
    "\n",
    "        self.reg_head = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(128, 4, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.feature_extractor(x)\n",
    "        cls_logits = self.cls_head(feat)\n",
    "        bbox_preds = self.reg_head(feat)\n",
    "        return cls_logits, bbox_preds\n",
    "\n",
    "# ----------- Step 3: Loss Functions -----------\n",
    "\n",
    "def detection_loss(cls_logits, bbox_preds, targets):\n",
    "    cls_loss_fn = nn.CrossEntropyLoss()\n",
    "    reg_loss_fn = nn.SmoothL1Loss()\n",
    "\n",
    "    batch_size = cls_logits.size(0)\n",
    "    cls_logits_flat = cls_logits.mean([2, 3])  # (B, num_classes)\n",
    "    labels = torch.stack([t['labels'][0] for t in targets if len(t['labels']) > 0])  # safer access\n",
    "    boxes = torch.stack([t['boxes'][0] for t in targets if len(t['boxes']) > 0])     # safer access\n",
    "\n",
    "    if labels.size(0) != cls_logits_flat.size(0):\n",
    "        labels = labels[:cls_logits_flat.size(0)]\n",
    "    if boxes.size(0) != cls_logits_flat.size(0):\n",
    "        boxes = boxes[:cls_logits_flat.size(0)]\n",
    "\n",
    "    cls_loss = cls_loss_fn(cls_logits_flat, labels)\n",
    "    box_preds_flat = bbox_preds.mean([2, 3])  # (B, 4)\n",
    "    reg_loss = reg_loss_fn(box_preds_flat, boxes)\n",
    "\n",
    "    return cls_loss + reg_loss\n",
    "\n",
    "# ----------- Step 4: Training -----------\n",
    "\n",
    "def train(model, dataloader, optimizer, device, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for images, targets in tqdm(dataloader):\n",
    "            images = images.to(device)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            cls_logits, bbox_preds = model(images)\n",
    "            loss = detection_loss(cls_logits, bbox_preds, targets[0])  # one by one\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}] Loss: {total_loss / len(dataloader):.4f}\")\n",
    "\n",
    "# ----------- Step 5: Evaluation & Visualization -----------\n",
    "\n",
    "def evaluate(model, dataloader, device, label_map={1: \"person\", 2: \"vehicle\"}):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (images, targets) in enumerate(tqdm(dataloader)):\n",
    "            images = images.to(device)\n",
    "            cls_logits, bbox_preds = model(images)\n",
    "\n",
    "            for j in range(images.size(0)):\n",
    "                img_np = (images[j].cpu().numpy().transpose(1, 2, 0) * 255).astype('uint8')\n",
    "                img_pil = Image.fromarray(img_np)\n",
    "                draw = ImageDraw.Draw(img_pil)\n",
    "\n",
    "                boxes = bbox_preds[j].mean([1, 2]).cpu()\n",
    "                scores = torch.softmax(cls_logits[j].mean([1, 2]).cpu(), dim=0)\n",
    "\n",
    "                if scores.max() > 0.5:\n",
    "                    label = scores.argmax().item()\n",
    "                    box = boxes.tolist()\n",
    "                    draw.rectangle(box, outline=\"red\", width=2)\n",
    "                    draw.text((box[0], box[1]), label_map.get(label, \"unknown\"), fill=\"white\")\n",
    "\n",
    "                img_pil.show()\n",
    "                if i > 2: return\n",
    "\n",
    "# ----------- Step 6: Custom Collate Function -----------\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    images = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "    images = torch.stack(images, dim=0)\n",
    "    return images, targets\n",
    "\n",
    "# ----------- Step 7: Pipeline -----------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    train_img_dir = \"/Volumes/Ayush/Projects/IR_Vehicle_Recognition/Dataset/FLIR_ADAS_v2/images_thermal_train\"\n",
    "    train_annot_file = \"/Volumes/Ayush/Projects/IR_Vehicle_Recognition/Dataset/FLIR_ADAS_v2/images_thermal_train/coco.json\"\n",
    "\n",
    "    val_img_dir = \"/Volumes/Ayush/Projects/IR_Vehicle_Recognition/Dataset/FLIR_ADAS_v2/images_thermal_val\"\n",
    "    val_annot_file = \"/Volumes/Ayush/Projects/IR_Vehicle_Recognition/Dataset/FLIR_ADAS_v2/images_thermal_val/coco.json\"\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    train_dataset = FLIRCOCODataset(train_img_dir, train_annot_file, transform=transform)\n",
    "    val_dataset = FLIRCOCODataset(val_img_dir, val_annot_file, transform=transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=custom_collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "    model = SimpleDetector(num_classes=3).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    train(model, train_loader, optimizer, device, epochs=10)\n",
    "    evaluate(model, val_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torchvision' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(torchvision\u001b[38;5;241m.\u001b[39m__version__)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torchvision' is not defined"
     ]
    }
   ],
   "source": [
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
